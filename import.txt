graph.py
CONTENT:
==================
import json
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END

from graphNetwork.nodes.initiate_func import initialize_agents_from_state
from graphNetwork.nodes.react_agent_node import call_agent
from graphNetwork.nodes.report_writer_node import report_writer
from graphNetwork.states import OverallState

load_dotenv()
llm = ChatOpenAI(model_name=os.getenv("MODEL"))

# Создаем подграф для взаимодействия агентов
app_builder = StateGraph(OverallState)

# Определяем агентов
agents = {
    "HR_Agent": call_agent,
    "BP_Agent": call_agent,
    "IT_Agent": call_agent,
    "KM_Agent": call_agent,
}

for agent_name, agent_function in agents.items():
    app_builder.add_node(agent_name, agent_function)

# Создаем связи между агентами (каждый может общаться с каждым)
for agent_from in agents.keys():
    for agent_to in agents.keys():
        if agent_from != agent_to:  # Исключаем самих себя
            app_builder.add_edge(agent_from, agent_to)

# Добавляем стартовый узел
app_builder.add_conditional_edges(START, initialize_agents_from_state, [agents.items()], "Report_Writer")

app_builder.add_node("Report_Writer", report_writer)
app_builder.add_edge("Report_Writer", END)

# Компилируем основной граф
graphNetwork = app_builder.compile()

# Генерация Mermaid-графа с улучшенным расположением и цветами
graph_image = graphNetwork.get_graph(
    xray=1,
).draw_mermaid_png()

# Сохранение графа
with open("network_graph_diagram.png", "wb") as file:
    file.write(graph_image)
print("Network graph diagram saved as 'network_graph_diagram.png'")

# Thread configuration and graph input
thread = {"configurable": {"thread_id": "1"}}

# Load from file
with open("../data/answer_1.json", "r") as file:
    data = json.load(file)

user_input = {
    "topic": "Help a multinational manufacturing company in their journey to product management maturity.",
    "questionnaire" :  data
}

response = graphNetwork.invoke(user_input, thread)

# Assuming the response is already generated
final_report = response.get("final_report")

if final_report:
    # Extract content from the final report
    report_content = final_report.content

    # Define the output file path
    output_file_path = "output.md"

    # Write the report content to the output.md file
    with open(output_file_path, "w", encoding="utf-8") as output_file:
        output_file.write(report_content)

    print(f"Final report has been written to {output_file_path}")
else:
    print("Final report is missing.")
==================

prompts.py
CONTENT:
==================
from langchain_core.prompts import ChatPromptTemplate

agent_prompt = ChatPromptTemplate.from_messages(
    [
        "system",
        "You work in a team on provided {topic}. Your goal is to review this topic and gice some recommendations"

    ]
)

==================

schemas.py
CONTENT:
==================
from typing import List

from pydantic import BaseModel, Field


class Agent(BaseModel):
    code_name: str = Field(
        description="Use only provided in system message names"
    )
    name: str = Field(
        description="The human-like name of the analyst persona. "
                    "Ensure the name is realistic and fits the persona."
    )
    role: str = Field(
        description="The specific role of the analyst in the context of the research topic. "
                    "Clearly defines the focus area such as 'Human Resources Analyst' or 'Business Process Analyst'."
    )
    description: str = Field(
        description="A detailed description of the analyst's focus, key competencies, tasks within the project, "
                    "concerns, and motives. This should align with the research topic and the analyst's role."
    )

    @property
    def persona(self) -> str:
        return f"Name: {self.name}\nRole: {self.role}\nDescription: {self.description}\n"


class Perspectives(BaseModel):
    agents: List[Agent] = Field(
        description="List of agents-analysts where each agents contains name, description and role",
    )


class Response(BaseModel):
    question: str = Field(
        description="Content of agent's current question."
    )

    next_agent: str = Field(
        description="Name of the next node to be called with question."
    )

    current_analysis: str = Field(
        description="Current analysis "
    )

==================

states.py
CONTENT:
==================
import operator
from typing import List, Annotated, Sequence, TypedDict, Union, Dict
from langgraph.graph import MessagesState
from langchain_core.agents import AgentAction, AgentFinish

from graphNetwork.schemas import Agent


class OverallState(MessagesState):
    topic: str
    questionnaire: str
    reviews: Annotated[List[str], operator.add]  # Four reviewers answers


class AgentState(MessagesState):
    code_name: str
    name: str
    description: str
    agent_topic: str
    agent_questionnaire: str
    # Local analysts messages
    current_analysis: str
    questions_asked: int

    messages: Annotated[List[str], operator.add]


    input: Dict[str, str]
    agent_outcome: Union[AgentAction, AgentFinish, None]
    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]

==================

react_agent_node.py
CONTENT:
==================
import os
from typing import Annotated, Literal

from dotenv import load_dotenv
from langchain_core.tools import tool, InjectedToolCallId
from langgraph.prebuilt import create_react_agent, InjectedState
from langchain_core.agents import AgentFinish
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain import hub
from langgraph.constants import END
from langgraph.prebuilt.chat_agent_executor import AgentState
from langgraph.types import Command
from langgraph.graph import StateGraph
from langgraph.prebuilt import ToolNode
from langchain_community.tools import TavilySearchResults


llm = ChatOpenAI(model_name=os.getenv("MODEL"))


def make_handoff_tool(*, agent_name: str):
    """Create a tool that can return handoff via a Command"""
    tool_name = f"transfer_to_{agent_name}"

    @tool(tool_name)
    def handoff_to_agent(state: Annotated[dict, InjectedState], tool_call_id: Annotated[str, InjectedToolCallId], ):
        """Ask another agent for help."""
        tool_message = {
            "role": "tool",
            "content": f"Successfully transferred to {agent_name}",
            "name": tool_name,
            "tool_call_id": tool_call_id,
        }

        print("handoff_to_agent " + agent_name + " has been invoked!")
        return Command(
            # navigate to another agent node in the PARENT graph
            goto=agent_name,
            graph=Command.PARENT,
            # This is the state update that the agent `agent_name` will see when it is invoked.
            # We're passing agent's FULL internal message history AND adding a tool message to make sure
            # the resulting chat history is valid.
            update={"messages": state["messages"] + [tool_message]},
        )

    return handoff_to_agent

@tool
def summary():
    """Summarizes provided text"""
    print("Report writer!")


def create_tools(code_name: str):
    handoff_tools = {
        "HR_Agent": make_handoff_tool(agent_name="HR_Agent"),
        "BP_Agent": make_handoff_tool(agent_name="BP_Agent"),
        "IT_Agent": make_handoff_tool(agent_name="IT_Agent"),
        "KM_Agent": make_handoff_tool(agent_name="KM_Agent"),
    }
    # Exclude the current agent from the tools
    return [TavilySearchResults(max_results=1)] + [summary] + [
        tool for name, tool in handoff_tools.items() if name != code_name
    ]

prompt = """
You are an agent {name} specializing in {role}. Your task is to assist in analyzing and resolving key issues related to the topic "{topic}". 
You have the following responsibilities:
{description}

Your team consists of: HR_Agent, BP_Agent, IT_Agent and KM_Agent

Perform an initial analysis based on the questions provided: {questions}.

If you need HR - recommendations, ask 'HR_Agent' for help.
If you need business processes recommendations, ask 'BP_Agent' for help.
If you need knowledge management recommendations, ask 'KM_Agent' for help.
If you need IT - recommendations, ask 'IT_Agent' for help.

You MUST include human-readable response before transferring to another agent.

When you have completed all questions. Summarize everything and provide a final analysis of the topic.

"""



def call_agent(state: AgentState) -> Command[Literal["HR_Agent", "BP_Agent", "IT_Agent", "KM_Agent", END]]:
    agent_prompt=prompt.format(
        name=state['name'],
        role=state['role'],
        topic=state['agent_topic'],
        questions=state['agent_questionnaire'],
        description=state['description'],
    )

    code_name = state["code_name"]
    tools = create_tools(code_name)  # Create tools dynamically, excluding the current agent

    agent = create_react_agent(llm, tools, state_modifier=agent_prompt)

    print("Agent " + code_name + " has been invoked!")

    # # Генерация Mermaid-графа с улучшенным расположением и цветами
    # graph_image = agent.get_graph(
    #     xray=2,
    # ).draw_mermaid_png()
    #
    # # Сохранение графа
    # with open("react_graph_diagram.png", "wb") as file:
    #     file.write(graph_image)
    # print("Network graph diagram saved as 'react_graph_diagram.png'")

    return agent.invoke(state)





















# "HR_Agent": reActAgent,
# "BP_Agent": reActAgent,
# "IT_Agent": reActAgent,
# "KM_Agent": reActAgent,


# from graphNetwork.schemas import Response
# from langchain_community.tools import TavilySearchResults
#
# from graphNetwork.states import OverallState, AgentState
#
# load_dotenv()
#
# prompt: PromptTemplate = hub.pull("hwchase17/react")
# # Основной промпт
#
# base_prompt = """
# You are an agent {name} specializing in {role}. Your task is to assist in analyzing and resolving key issues related to the topic "{topic}".
# You have the following responsibilities:
# {description}
# """
# # Дополнительные кусочки промпта
#
#
# additional_prompts = {
#     "initial_analysis": "\nPerform an initial analysis based on the questions provided: {questions}.",
#     "respond_to_question": "\nYou received a question from {sender}. Please answer it using your expertise.",
#     "final_analysis": "\nYou have completed all questions. Summarize your insights and provide a final analysis of the topic.",
# }
#
# structured_llm = llm.with_structured_output(Response)
#
# react_agent_runnable = create_react_agent(tools=tools, llm=llm, prompt=prompt)
#
#
# tool_executor = ToolNode(tools)
# # Функция для построения текущего промпта
#
#
# def build_agent_prompt(state: AgentState) -> str:
#     """
#     Создает текущий промпт для агента, основываясь на его состоянии и контексте.
#     """
#
#     prompt = base_prompt.format(
#         role=state["role"],
#         name=state["name"],
#         description=state["description"],
#         topic=state["topic"],
#     )
#
#     if state["questions_asked"] < 1:
#         prompt = prompt + additional_prompts["initial_analysis"].format()
#
#     if state["questions_asked"] >= 3:
#         prompt = prompt + additional_prompts["final_analysis"].format()
#
#     return prompt
#
#
# def run_agent_reasoning_engine(state: AgentState):
#
#
#
#
#
#
#     agent_outcome = react_agent_runnable.invoke(state)
#     return {"agent_outcome": agent_outcome}
#
#
# # def create_react_graph(state):
# #     AGENT_REASON = "agent_reason"
# #     ACT = "act"
# #
# #     def should_continue(state: AgentState)->str:
# #         if isinstance(state["agent_outcome"], AgentFinish):
# #             return END
# #         return ACT
# #
# #     flow = StateGraph(AgentState)
# #
# #     flow.add_node(AGENT_REASON, run_agent_reasoning_engine)
# #     flow.set_entry_point(AGENT_REASON)
# #     flow.add_node(ACT, execute_tools)
# #
# #     flow.add_conditional_edges(AGENT_REASON, should_continue)
# #
# #     flow.add_edge(ACT, AGENT_REASON)
# #
# #     return flow.compile()
#
# def execute_tools(state: AgentState):
#     agent_action = state["agent_outcome"]
#     output = tool_executor.invoke(agent_action)
#     return {"intermediate_steps": [(agent_action, str(output))]}
#

==================

report_writer_node.py
CONTENT:
==================
# Финальный сбор отчета
from graphNetwork.states import OverallState


def report_writer(state: OverallState):
    # Сбор и вывод финального результата
    pass
==================

initiate_func.py
CONTENT:
==================
import os

from dotenv import load_dotenv
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.types import Send

from graphNetwork.schemas import Perspectives
from graphNetwork.states import OverallState


load_dotenv()
llm = ChatOpenAI(model_name=os.getenv("MODEL"))

agents_creation_instructions = """
You are tasked with creating AI agents-analysts. Follow these instructions:
Use provided in prompts names
1. Review the provided research topic.
2. Generate four research teams strictly using provided names:
    a. **HR_Agent**: Focused on HR issues like team dynamics, performance, and training.
    b. **BP_Agent**: Specializing in process optimization and automation.
    c. **KM_Agent**: Concentrating on knowledge sharing and tools.
    d. **IT_Agent**: Addressing IT strategies and tools.
3. Each agent must have explicitly provided name, role and description reflecting their responsibilities.
"""

def initialize_agents_from_state(state: OverallState):
    """
    Initializes agents based on the provided overall state.
    Combines agent creation and initialization.
    """
    topic = state["topic"]
    questionnaire = state["questionnaire"]
    questionnaire = "Here should be questions, but now this is just a test!"

    print(f"Initializing agents for topic:\n\t{topic}")

    # Use LLM to generate structured agent configurations
    structured_llm = llm.with_structured_output(Perspectives)
    system_message = SystemMessage(content=agents_creation_instructions)
    human_message = HumanMessage(content=f"Generate the agents for the topic: {topic}.")

    # Generate agent configurations
    perspectives: Perspectives = structured_llm.invoke([system_message, human_message])

    # Serialize agent data
    serialized_agents = [
        {
            "code_name": agent.code_name,
            "name": agent.name,
            "role": agent.role,
            "description": agent.description
        }
        for agent in perspectives.agents
    ]

    # Return initialized agents (could also return "Send" objects if needed)
    return [
        Send(
            agent["code_name"],
            {
                "code_name": agent["code_name"],
                "name": agent["name"],
                "description": agent["description"],
                "role": agent["role"],
                "agent_topic": topic,
                "agent_questionnaire": questionnaire,
                "messages": [],
                "current_analysis": "",
                "questions_asked": 0,
            }
        )
        for agent in serialized_agents
    ]
==================

